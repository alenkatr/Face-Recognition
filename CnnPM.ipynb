{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dominican-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/facebookresearch/poincare-embeddings/tree/main/hype/manifolds\n",
    "#for poincare embeddings\n",
    "#maybe look at https://github.com/drewwilimitis/hyperbolic-learning\n",
    "#maybe: https://lars76.github.io/2020/07/24/implementing-poincare-embedding.html\n",
    "#maybe: https://pypi.org/project/hyperlib/\n",
    "#maybe this is better: https://github.com/leymir/hyperbolic-image-embeddings/blob/master/hyptorch/nn.py\n",
    "#more  about python classes: https://jovian.ai/aakashns/03-logistic-regression#C50\n",
    "#https://www.w3schools.com/python/python_classes.asp\n",
    "\n",
    "#CnnEmbedding.ipynb\n",
    "#My CNN (embedded in hyp.space) for attributes + Knn\n",
    "#it works but...-----anyway: still not know if ot is good, because I define args.dim and other........??????????\n",
    "#train data is too big to compute here\n",
    "#https://github.com/leymir/hyperbolic-image-embeddings/blob/master/examples/mnist.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import NN as hypnn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets, neighbors, metrics \n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "comparative-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a CNN\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n",
    "        self.fc2 = nn.Linear(500, 600)  #here were self.fc2 = nn.Linear(500, args.dim)\n",
    "        self.tp = hypnn.ToPoincare(c=-1, train_x=600, train_c=600, ball_dim=600)\n",
    "        #up were self.tp = hypnn.ToPoincare(c=args.c, train_x=args.train_x, train_c=args.train_c, ball_dim=args.dim)\n",
    "        self.mlr = hypnn.HyperbolicMLR(ball_dim=600, n_classes=10, c=-1)\n",
    "        # Define proportion or neurons to dropout\n",
    "        self.dropout = nn.Dropout(0.5)         # Apply dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4 * 4 * 50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)        # Apply dropout\n",
    "        x = self.fc2(x)\n",
    "        x = self.tp(x)\n",
    "        return x #F.log_softmax(self.mlr(x, c=self.tp.c), dim=-1)   #maybe only return x, then X=np.array(x)??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "organized-screening",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOADING DATASET---mnist dataset\n",
    "\n",
    "\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dedicated-pastor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKING DATASET ITERABLE---.....\n",
    "\n",
    "batch_size = 100\n",
    "\"\"\"\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\"\"\"\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "framed-reader",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n",
      "100\n",
      "(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
      "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
      "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
      "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
      "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
      "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
      "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
      "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
      "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
      "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
      "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
      "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
      "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
      "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]]), 5)\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(test_loader))\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "juvenile-humanity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=600, bias=True)\n",
       "  (tp): ToPoincare(\n",
       "    c=Parameter containing:\n",
       "    tensor([-1.], requires_grad=True), train_x=600\n",
       "  )\n",
       "  (mlr): HyperbolicMLR(Poincare ball dim=600, n_classes=10, c=-1)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelH = Net(10)\n",
    "modelH.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "amino-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the attributes----------------\n",
    "# Select the desired layer---in this ex. this is fc2\n",
    "layer = model._modules.get('dropout')\n",
    "outputs = []\n",
    "\n",
    "#copy features from penultimate layer (model, input, output)\n",
    "def copy_features(m, i, o):\n",
    "    o = o[:, :].detach().numpy().tolist()\n",
    "    outputs.append(o)\n",
    "    #o = o[:, :, 0, 0].detach().numpy().tolist() #use this, get error in _ = model(X): IndexError: too many indices for tensor of dimension 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spoken-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define and attach hook to the penulimate layer\n",
    "_ = layer.register_forward_hook(copy_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "owned-cleaner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# Generate image's features for all images in dloader and saves them in the list outputs\n",
    "for XX, yy in test_loader:\n",
    "    _ = model(XX)\n",
    "print(len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "photographic-hacker",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 500)\n",
      "[0.08901027 0.         0.03132187 0.         0.04969964 0.0692743\n",
      " 0.         0.08693587 0.         0.         0.14620221 0.\n",
      " 0.08679606 0.         0.         0.00780601 0.         0.\n",
      " 0.0797523  0.         0.         0.05608506 0.         0.\n",
      " 0.         0.0541016  0.         0.         0.         0.\n",
      " 0.         0.04857452 0.01219959 0.         0.0092326  0.\n",
      " 0.         0.07752223 0.         0.06060091 0.         0.00043647\n",
      " 0.         0.         0.         0.01401155 0.         0.00624087\n",
      " 0.02161368 0.         0.04952208 0.0087075  0.         0.0666145\n",
      " 0.07864208 0.         0.         0.04007846 0.         0.05858412\n",
      " 0.12840928 0.         0.0621311  0.         0.04460188 0.\n",
      " 0.08023936 0.         0.         0.04654349 0.098539   0.\n",
      " 0.04274815 0.         0.03998552 0.         0.         0.04019527\n",
      " 0.         0.01041107 0.         0.         0.06077857 0.0866178\n",
      " 0.00647829 0.11181688 0.         0.03800711 0.         0.\n",
      " 0.03978224 0.08097462 0.07297568 0.         0.06502941 0.01112071\n",
      " 0.         0.         0.         0.01115273 0.         0.\n",
      " 0.02295218 0.06029454 0.         0.05973731 0.08561364 0.08644319\n",
      " 0.         0.18436596 0.13039018 0.         0.         0.\n",
      " 0.05870343 0.         0.         0.01316434 0.         0.02902618\n",
      " 0.         0.0333735  0.         0.01807879 0.         0.\n",
      " 0.00263018 0.         0.12961136 0.11234853 0.         0.\n",
      " 0.         0.         0.         0.         0.04580459 0.05627378\n",
      " 0.05574409 0.0051175  0.09452176 0.03170836 0.         0.\n",
      " 0.         0.05822095 0.         0.07155573 0.         0.\n",
      " 0.         0.00084813 0.06526849 0.         0.11631909 0.\n",
      " 0.         0.         0.08684169 0.04719616 0.10743459 0.0210632\n",
      " 0.06655768 0.         0.         0.03862542 0.         0.02409244\n",
      " 0.04234072 0.         0.         0.         0.         0.\n",
      " 0.05642092 0.11442067 0.07995388 0.         0.07079367 0.01551173\n",
      " 0.         0.07767346 0.         0.01382765 0.06715656 0.01683556\n",
      " 0.11776766 0.0050072  0.0774461  0.         0.         0.12408131\n",
      " 0.         0.04406901 0.         0.         0.         0.\n",
      " 0.         0.         0.10723998 0.         0.00894099 0.\n",
      " 0.00508493 0.03307812 0.         0.         0.         0.10672267\n",
      " 0.04906064 0.         0.         0.06742569 0.06949134 0.0790987\n",
      " 0.01733349 0.03488071 0.         0.         0.         0.07572559\n",
      " 0.02201792 0.         0.         0.04894162 0.         0.\n",
      " 0.02501747 0.0180023  0.         0.07343816 0.         0.04008687\n",
      " 0.161422   0.         0.09482948 0.05290235 0.09402166 0.\n",
      " 0.01513246 0.         0.         0.06478239 0.00075371 0.12488327\n",
      " 0.05251613 0.         0.05890102 0.02939007 0.06949582 0.\n",
      " 0.         0.012089   0.02471436 0.13194735 0.         0.02388089\n",
      " 0.         0.         0.03689885 0.02484298 0.13695025 0.\n",
      " 0.         0.         0.         0.         0.         0.03338685\n",
      " 0.07793537 0.02051292 0.01681425 0.10421428 0.         0.05239708\n",
      " 0.         0.04727598 0.04312418 0.         0.         0.\n",
      " 0.05759981 0.03133804 0.         0.         0.14143512 0.\n",
      " 0.         0.         0.06082023 0.07377529 0.05566925 0.06226005\n",
      " 0.         0.         0.00514416 0.         0.13261999 0.\n",
      " 0.08884736 0.06411696 0.03522456 0.03209389 0.04318934 0.00399489\n",
      " 0.00496599 0.00562963 0.         0.00426368 0.         0.0183403\n",
      " 0.         0.00853338 0.         0.         0.         0.0452975\n",
      " 0.         0.         0.         0.0918947  0.         0.00769933\n",
      " 0.02004327 0.02015127 0.         0.01340594 0.         0.\n",
      " 0.00683186 0.         0.02926545 0.08629151 0.07854788 0.03489952\n",
      " 0.         0.06141375 0.         0.         0.02333948 0.11413652\n",
      " 0.         0.         0.         0.02428988 0.         0.08431404\n",
      " 0.         0.037958   0.0241412  0.03234785 0.         0.00441875\n",
      " 0.03016672 0.03059172 0.         0.         0.00223526 0.01011208\n",
      " 0.01083991 0.         0.06810662 0.11078476 0.         0.03695432\n",
      " 0.         0.02854021 0.06622513 0.04050819 0.04149878 0.02493237\n",
      " 0.         0.06058781 0.         0.01141424 0.05400305 0.06915052\n",
      " 0.         0.         0.         0.         0.         0.03208717\n",
      " 0.         0.         0.02418986 0.         0.         0.\n",
      " 0.02511236 0.14965504 0.         0.0440575  0.03447439 0.\n",
      " 0.         0.02932698 0.01628561 0.05742985 0.         0.02577006\n",
      " 0.19179863 0.         0.1402733  0.         0.         0.01124839\n",
      " 0.         0.04453189 0.         0.         0.         0.07947189\n",
      " 0.01274709 0.         0.         0.         0.08080572 0.02583189\n",
      " 0.09254262 0.00998797 0.11238035 0.         0.05806737 0.\n",
      " 0.05299307 0.         0.         0.03022796 0.05197401 0.06346917\n",
      " 0.         0.         0.08500402 0.02107176 0.         0.\n",
      " 0.         0.04911621 0.         0.         0.         0.\n",
      " 0.         0.05210378 0.00593411 0.02341555 0.03713471 0.\n",
      " 0.         0.         0.         0.         0.         0.01617901\n",
      " 0.         0.04133411 0.         0.01386438 0.00945996 0.08851756\n",
      " 0.03538758 0.00882087 0.         0.03348214 0.         0.\n",
      " 0.         0.00353835 0.01073886 0.09330962 0.05736545 0.\n",
      " 0.04446566 0.         0.         0.         0.         0.\n",
      " 0.         0.0634189  0.         0.02665566 0.         0.\n",
      " 0.02184909 0.07846954 0.01566331 0.         0.         0.08688148\n",
      " 0.00513263 0.         0.         0.04971333 0.11957212 0.\n",
      " 0.02611964 0.03334474]\n"
     ]
    }
   ],
   "source": [
    "# flatten list of features to remove batches!!!!!!!!!!!!!!----in numpy array....\n",
    "list_features = [item for sublist in outputs for item in sublist]\n",
    "X = np.array(list_features)\n",
    "print(X.shape)\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "illegal-beverage",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('mnist_test.csv')\n",
    "X = np.array(list_features)\n",
    "y = np.array(train_df.iloc[:, 1])   #all rows & only first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "colored-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poincare_distance(row1, row2):\n",
    "    #print('len(row1) = ',len(row1))\n",
    "    #print('len(row1) = ',len(row1))\n",
    "    distance = 0.0\n",
    "    d = 0.0\n",
    "    norm1 = 0.0\n",
    "    norm2 = 0.0\n",
    "    \n",
    "    d = (row1 - row2)*(row1 - row2)\n",
    "    d = d.sum()\n",
    "    \n",
    "    norm1 = row1 * row1\n",
    "    norm1 = norm1.sum()\n",
    "    \n",
    "    norm2 = row2 * row2\n",
    "    norm2 = norm2.sum()\n",
    "    \n",
    "    m = 1 + 2*d/((1-norm1)*(1-norm2))\n",
    "    distance = np.log(m + np.sqrt(m*m - 1))\n",
    "    \n",
    "    return(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-radical",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alenkat/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in log\n",
      "/home/alenkat/.local/lib/python3.6/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in log\n"
     ]
    }
   ],
   "source": [
    "k_scoresP = []\n",
    "for k in range(1,5):\n",
    "    knn = neighbors.KNeighborsClassifier(n_neighbors=k, leaf_size=3, metric='pyfunc', metric_params={\"func\":poincare_distance}, n_jobs=3)\t#to add new metric: metric='pyfunc'\n",
    "    #for testing\n",
    "    #knn = neighbors.KNeighborsClassifier(n_neighbors=5, algorithm='brute', metric='euclidean')    #algorithm='ball_tree' - for a custom metric\n",
    "    #fit the model\n",
    "    knn.fit(X,y)\n",
    "    #cross validation-------------------------------after knn\n",
    "    scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "    k_scoresP.append(scores.mean())\n",
    "print(k_scoresP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Maximum accuracy:\",max(k_scoresEM),\"at k =\",k_scoresEM.index(max(k_scoresEM))+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
